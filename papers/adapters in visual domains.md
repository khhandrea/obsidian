---
annotation-target: https://arxiv.org/pdf/1705.08045.pdf
tags:
  - paper
---


>%%
>```annotation-json
>{"created":"2023-09-16T05:50:53.118Z","updated":"2023-09-16T05:50:53.118Z","document":{"title":"1705.08045.pdf","link":[{"href":"urn:x-pdf:b438d3eac921c2d1e5104b07be5eee94"},{"href":"https://arxiv.org/pdf/1705.08045.pdf"}],"documentFingerprint":"b438d3eac921c2d1e5104b07be5eee94"},"uri":"https://arxiv.org/pdf/1705.08045.pdf","target":[{"source":"https://arxiv.org/pdf/1705.08045.pdf","selector":[{"type":"TextPositionSelector","start":1888,"end":2089},{"type":"TextQuoteSelector","exact":"we explore the second challenge and look at how deep learning techniques can be usedto learn universal representations [5], i.e. feature extractors that can work well in several differentimage domains.","prefix":"tons, etc (fig. 1).In this work ","suffix":" We refer to this problem as mul"}]}]}
>```
>%%
>*%%PREFIX%%tons, etc (fig. 1).In this work%%HIGHLIGHT%% ==we explore the second challenge and look at how deep learning techniques can be usedto learn universal representations [5], i.e. feature extractors that can work well in several differentimage domains.== %%POSTFIX%%We refer to this problem as mul*
>%%LINK%%[[#^egetq8jfhoq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^egetq8jfhoq


>%%
>```annotation-json
>{"created":"2023-09-16T06:02:05.904Z","updated":"2023-09-16T06:02:05.904Z","document":{"title":"1705.08045.pdf","link":[{"href":"urn:x-pdf:b438d3eac921c2d1e5104b07be5eee94"},{"href":"https://arxiv.org/pdf/1705.08045.pdf"}],"documentFingerprint":"b438d3eac921c2d1e5104b07be5eee94"},"uri":"https://arxiv.org/pdf/1705.08045.pdf","target":[{"source":"https://arxiv.org/pdf/1705.08045.pdf","selector":[{"type":"TextPositionSelector","start":2310,"end":2343},{"type":"TextQuoteSelector","exact":"can learn well from many domains.","prefix":"ne is to develop algorithmsthat ","suffix":" If domains are learned sequenti"}]}]}
>```
>%%
>*%%PREFIX%%ne is to develop algorithmsthat%%HIGHLIGHT%% ==can learn well from many domains.== %%POSTFIX%%If domains are learned sequenti*
>%%LINK%%[[#^rzv8ec81a5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rzv8ec81a5


>%%
>```annotation-json
>{"created":"2023-09-16T06:03:31.968Z","updated":"2023-09-16T06:03:31.968Z","document":{"title":"1705.08045.pdf","link":[{"href":"urn:x-pdf:b438d3eac921c2d1e5104b07be5eee94"},{"href":"https://arxiv.org/pdf/1705.08045.pdf"}],"documentFingerprint":"b438d3eac921c2d1e5104b07be5eee94"},"uri":"https://arxiv.org/pdf/1705.08045.pdf","target":[{"source":"https://arxiv.org/pdf/1705.08045.pdf","selector":[{"type":"TextPositionSelector","start":2856,"end":2958},{"type":"TextQuoteSelector","exact":" while making sure that it still performs well onthe original domain, i.e. to learn without forgetting","prefix":" domain to another, but to do so","suffix":" [21].The second challenge of mu"}]}]}
>```
>%%
>*%%PREFIX%%domain to another, but to do so%%HIGHLIGHT%% ==while making sure that it still performs well onthe original domain, i.e. to learn without forgetting== %%POSTFIX%%[21].The second challenge of mu*
>%%LINK%%[[#^1a5y0ginsew|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1a5y0ginsew


>%%
>```annotation-json
>{"created":"2023-09-16T06:06:02.983Z","updated":"2023-09-16T06:06:02.983Z","document":{"title":"1705.08045.pdf","link":[{"href":"urn:x-pdf:b438d3eac921c2d1e5104b07be5eee94"},{"href":"https://arxiv.org/pdf/1705.08045.pdf"}],"documentFingerprint":"b438d3eac921c2d1e5104b07be5eee94"},"uri":"https://arxiv.org/pdf/1705.08045.pdf","target":[{"source":"https://arxiv.org/pdf/1705.08045.pdf","selector":[{"type":"TextPositionSelector","start":2988,"end":3013},{"type":"TextQuoteSelector","exact":"multiple-domain learning,","prefix":"ng [21].The second challenge of ","suffix":" and our main concern in this pa"}]}]}
>```
>%%
>*%%PREFIX%%ng [21].The second challenge of%%HIGHLIGHT%% ==multiple-domain learning,== %%POSTFIX%%and our main concern in this pa*
>%%LINK%%[[#^cv8e1cwr2m|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^cv8e1cwr2m
