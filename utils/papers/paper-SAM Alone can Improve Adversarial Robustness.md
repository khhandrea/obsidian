---
created-at: 2023-11-08T00:45
tags:
  - paper
description: SAM and adversarial robustness
annotation-target: https://arxiv.org/pdf/2305.05392.pdf
---
# Sharpness-Aware Minimization Alone can Improve Adversarial Robustness

>%%
>```annotation-json
>{"created":"2023-11-08T03:04:48.836Z","updated":"2023-11-08T03:04:48.836Z","document":{"title":"","link":[{"href":"urn:x-pdf:c541d72138362755c8312bc586b5bdc8"},{"href":"https://arxiv.org/pdf/2305.05392.pdf"}],"documentFingerprint":"c541d72138362755c8312bc586b5bdc8"},"uri":"https://arxiv.org/pdf/2305.05392.pdf","target":[{"source":"https://arxiv.org/pdf/2305.05392.pdf","selector":[{"type":"TextPositionSelector","start":358,"end":490},{"type":"TextQuoteSelector","exact":"We find that using only SAMcan achieve superior adversarial robustnesswithout sacrificing clean accuracy comparedto standard trainin","prefix":"ext of adver-sarial robustness. ","suffix":"g, which is an unexpectedbenefit"}]}]}
>```
>%%
>*%%PREFIX%%ext of adver-sarial robustness.%%HIGHLIGHT%% ==We find that using only SAMcan achieve superior adversarial robustnesswithout sacrificing clean accuracy comparedto standard trainin== %%POSTFIX%%g, which is an unexpectedbenefit*
>%%LINK%%[[#^znxksnvr5j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^znxksnvr5j


>%%
>```annotation-json
>{"created":"2023-11-08T03:04:54.762Z","updated":"2023-11-08T03:04:54.762Z","document":{"title":"","link":[{"href":"urn:x-pdf:c541d72138362755c8312bc586b5bdc8"},{"href":"https://arxiv.org/pdf/2305.05392.pdf"}],"documentFingerprint":"c541d72138362755c8312bc586b5bdc8"},"uri":"https://arxiv.org/pdf/2305.05392.pdf","target":[{"source":"https://arxiv.org/pdf/2305.05392.pdf","selector":[{"type":"TextPositionSelector","start":524,"end":593},{"type":"TextQuoteSelector","exact":"We also discuss the relation betweenSAM and adversarial training (AT)","prefix":" which is an unexpectedbenefit. ","suffix":", a popularmethod for improving "}]}]}
>```
>%%
>*%%PREFIX%%which is an unexpectedbenefit.%%HIGHLIGHT%% ==We also discuss the relation betweenSAM and adversarial training (AT)== %%POSTFIX%%, a popularmethod for improving*
>%%LINK%%[[#^40y8onrz02|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^40y8onrz02


>%%
>```annotation-json
>{"created":"2023-11-08T05:02:45.876Z","updated":"2023-11-08T05:02:45.876Z","document":{"title":"","link":[{"href":"urn:x-pdf:c541d72138362755c8312bc586b5bdc8"},{"href":"https://arxiv.org/pdf/2305.05392.pdf"}],"documentFingerprint":"c541d72138362755c8312bc586b5bdc8"},"uri":"https://arxiv.org/pdf/2305.05392.pdf","target":[{"source":"https://arxiv.org/pdf/2305.05392.pdf","selector":[{"type":"TextPositionSelector","start":11885,"end":12322},{"type":"TextQuoteSelector","exact":"we first emphasize that both tech-niques involve adding perturbation as a form of data aug-mentation for eliminating non-robust features (Ilyas et al.,2019). However, AT explicitly adds these perturbationsto input examples, while SAM focuses on perturbing theparameters, which can be considered an implicit kind ofdata augmentation on the feature space. Therefore, bothtechniques involve perturbation on features, but in differentspaces.","prefix":"6)To illustrate their relation, ","suffix":"To be more specific and formal, "}]}]}
>```
>%%
>*%%PREFIX%%6)To illustrate their relation,%%HIGHLIGHT%% ==we first emphasize that both tech-niques involve adding perturbation as a form of data aug-mentation for eliminating non-robust features (Ilyas et al.,2019). However, AT explicitly adds these perturbationsto input examples, while SAM focuses on perturbing theparameters, which can be considered an implicit kind ofdata augmentation on the feature space. Therefore, bothtechniques involve perturbation on features, but in differentspaces.== %%POSTFIX%%To be more specific and formal,*
>%%LINK%%[[#^574sq77he3u|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^574sq77he3u
